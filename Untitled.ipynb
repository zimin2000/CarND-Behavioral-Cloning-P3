{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Flatten, Dense, Lambda, Cropping2D, Convolution2D, MaxPooling2D, Dropout, Activation, SpatialDropout2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "CHECKPOINT = \"model.checkpoint.h5\"\n",
    "MODEL = \"model.h5\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "IMG_WIDTH = 320\n",
    "IMG_HEIGHT = 160\n",
    "\n",
    "CROP_TOP = 50\n",
    "CROP_BOTTOM = 20\n",
    "CROP_SIDE = 1\n",
    "\n",
    "samples = []\n",
    "train_samples, validation_samples = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The goal of this function is to assign some reasonable steering to frames that otherwise\n",
    "# has zero steering.\n",
    "#\n",
    "# So, for zero the steering frame that is right before a non-zero steering, a 80% of the\n",
    "# non-zero steering is assigned.\n",
    "#\n",
    "def assign_non_zero(data):\n",
    "\n",
    "    prev_steering = 0.\n",
    "\n",
    "    for i in reversed(range(len(data))):\n",
    "        if data[i]['steering'] == 0.0:\n",
    "\n",
    "            if prev_steering > 0.:\n",
    "                data[i]['steering'] = prev_steering * 0.8\n",
    "                prev_steering = 0.\n",
    "\n",
    "        else:\n",
    "            prev_steering = data[i]['steering']\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# As zero steering samples are severely over-represented, triplicate the non-zero once.\n",
    "#\n",
    "# Note that the generator will randomize the images to get different pictures.\n",
    "#\n",
    "def triple_non_zero(data):\n",
    "    non_zero = filter((lambda item: abs(item['steering']) > 0.), data)\n",
    "\n",
    "    return data + non_zero + non_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Sample generator can generate 3 x 2 x 3x3 = 54 different samples.\n",
    "# It is used not to keep the pictures in memory (paying for this by\n",
    "# loading them every time).\n",
    "#\n",
    "def generator(samples, batch_size=BATCH_SIZE):\n",
    "    num_samples = len(samples)\n",
    "\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "\n",
    "            for sample in batch_samples:\n",
    "\n",
    "                #\n",
    "                # With 60% chance take central camera and with 20% take left\n",
    "                # or right cameras.\n",
    "                #\n",
    "                # For central camera take steering as is. For left and right\n",
    "                # cameras add 0.20 and -0.20 to the steering respectively.\n",
    "                choice = np.random.randint(100)\n",
    "\n",
    "                if choice < 60:\n",
    "                    image_name = sample['center']\n",
    "                    angle = sample['steering']\n",
    "\n",
    "                elif choice < 80:\n",
    "                    image_name = sample['left']\n",
    "                    angle = sample['steering'] + 0.20\n",
    "\n",
    "                else:\n",
    "                    image_name = sample['right']\n",
    "                    angle = sample['steering'] - 0.20\n",
    "\n",
    "                image = plt.imread(image_name)\n",
    "\n",
    "                choice = np.random.randint(100)\n",
    "\n",
    "                #\n",
    "                # With 50% chance flip the image vertically.\n",
    "                if choice < 50:\n",
    "                    image = image[:,::-1,:]\n",
    "                    angle = -angle\n",
    "\n",
    "                #\n",
    "                # Slightly move the image (+/-1px vertically and horizontally).\n",
    "                # 68% - dx/dy=0, 16% dx/dy = +/-1\n",
    "                dx = np.clip(int(np.random.normal()*7),-7,7)\n",
    "                dy = np.clip(int(np.random.normal()*7),-7,7)\n",
    "                \n",
    "                image = np.roll(np.roll(image, dx, axis=1), dy, axis=0)\n",
    "\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "\n",
    "            yield (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(DATA_DIR, has_header=True):\n",
    "    data = []\n",
    "    columns = [\"center\",\"left\",\"right\",\"steering\",\"throttle\",\"brake\",\"speed\"]\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"driving_log.csv\")) as FILE:\n",
    "        reader = csv.reader(FILE)\n",
    "\n",
    "        if has_header:\n",
    "            if reader.next() != columns:\n",
    "                raise Exception('Unexpected set of columns.')\n",
    "\n",
    "        for values in reader:\n",
    "            if len(values) != len(columns):\n",
    "                raise Exception('Column number missmatch.')\n",
    "\n",
    "            for i in range(3):\n",
    "                values[i] = os.path.join(DATA_DIR, 'IMG', values[i].split('/')[-1])\n",
    "\n",
    "            for i in range(3, 7):\n",
    "                values[i] = float(values[i])\n",
    "\n",
    "            data.append(dict(zip(columns, values)))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Preprocessing 0.1: Cropping useless top and bottom pixels\n",
    "    # 3 @ 320 x 160 -> 3 @ 318 x 90\n",
    "    model.add(Cropping2D(cropping=((CROP_TOP, CROP_BOTTOM), (CROP_SIDE, CROP_SIDE)), input_shape=input_shape, dim_ordering='tf'))\n",
    "\n",
    "    # Preprocessing 0.2: Centered around zero with small standard deviation\n",
    "    # 3 @ 318 x 90 -> 3 @ 318 x 90\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5))\n",
    "\n",
    "\n",
    "    # Convolutional Layer 1:\n",
    "    # 3 @ 318 x 90 -> Conv 7 x 7 (+1 x +1) -> MaxPool 3 x 3 -> Dropout -> Elu -> 24 @ 104 x 28\n",
    "    model.add(Convolution2D(24, 7, 7, border_mode='valid'))\n",
    "    model.add(MaxPooling2D((3, 3)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Convolutional Layer 2:\n",
    "    # 24 @ 104 x 28 -> Conv 5 x 5 (+1 x +1) -> MaxPool 2 x 2 -> Dropout -> Elu -> 36 @ 50 x 12\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode='valid'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Convolutional Layer 3:\n",
    "    # 36 @ 50 x 12 -> Conv 5 x 5 (+1 x +1) -> MaxPool 2 x 2 -> Dropout -> Elu -> 48 @ 23 x 4\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode='valid'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Convolutional Layer 4:\n",
    "    # 48 @ 23 x 4 -> Conv 3 x 3 -> Dropout -> Elu -> 64 @ 21 x 2\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Convolutional Layer 5:\n",
    "    # 64 @ 21 x 2 -> Conv 3 x 2 -> Dropout -> Elu -> 64 @ 19 x 1\n",
    "    model.add(Convolution2D(64, 2, 3, border_mode='valid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # FC Layer 1:\n",
    "    # 64 @ 19 x 1 -> Flatten -> Dropout -> Elu -> 1216\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # FC Layer 2:\n",
    "    # 1216 -> Dropout -> Elu -> 100\n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # FC Layer 3:\n",
    "    # 100 -> Dropout -> Elu -> 50\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # FC Layer 4:\n",
    "    # 50 -> Dropout -> 10\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Output Layer:\n",
    "    # 10 -> Dense -> 1\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_samples():\n",
    "    global samples_1, samples_2, samples_3\n",
    "    global train_samples, validation_samples\n",
    "\n",
    "    orig_samples = []\n",
    "    orig_samples = read_data(\"./data\")\n",
    "\n",
    "    extra_samples = []\n",
    "#    extra_samples = read_data(\"./extra_data\", has_header=False)\n",
    "\n",
    "    samples = orig_samples + extra_samples\n",
    "    samples_1 = copy.deepcopy(samples)\n",
    "\n",
    "    print \"Original:       \\t total: {}, non-zero: {}\".format(len(samples), len(filter(lambda i: i['steering'] != 0.00, samples)))\n",
    "\n",
    "    samples = assign_non_zero(samples)\n",
    "    samples_2 = copy.deepcopy(samples)\n",
    "\n",
    "    print \"Assign non-zero:\\t total: {}, non-zero: {}\".format(len(samples), len(filter(lambda i: i['steering'] != 0.00, samples)))\n",
    "\n",
    "    samples = triple_non_zero(samples)\n",
    "    samples_3 = copy.deepcopy(samples)\n",
    "\n",
    "    print \"Drop low:       \\t total: {}, non-zero: {}\".format(len(samples), len(filter(lambda i: i['steering'] != 0.00, samples)))\n",
    "\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "    print \"training: {}, validation: {}\".format(len(train_samples), len(validation_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(load_checkpoint = False):\n",
    "\n",
    "    model = build_model((IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    if load_checkpoint:\n",
    "        model = load_model(\"model.checkpoint.h5\")\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=\"model.checkpoint.h5\", verbose=1, save_best_only=True)\n",
    "\n",
    "    model.fit_generator(generator = generator(train_samples, batch_size = BATCH_SIZE),\n",
    "                        samples_per_epoch = len(train_samples),\n",
    "                        validation_data = generator(validation_samples, batch_size = BATCH_SIZE),\n",
    "                        nb_val_samples = len(validation_samples),\n",
    "                        nb_epoch = 25, \n",
    "                        verbose = 2,\n",
    "                        callbacks = [checkpointer])\n",
    "\n",
    "    model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       \t total: 8036, non-zero: 3675\n",
      "Assign non-zero:\t total: 8036, non-zero: 3945\n",
      "Drop low:       \t total: 15926, non-zero: 11835\n",
      "training: 12740, validation: 3186\n"
     ]
    }
   ],
   "source": [
    "prepare_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: min=-0.9426954, max=1.0, mean=0.00406964406483, sigma=0.128832055818\n",
      "Assign non-zero: min=-0.9426954, max=1.0, mean=0.00707090503825, sigma=0.130754539596\n",
      "Drop low: min=-0.9426954, max=1.0, mean=0.0107035902714, sigma=0.160752420688\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAFkCAYAAABcjHpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEaKTHgRBEARBEARBEAQ9wv8DdJ4Jkh5jESUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f708ae39810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for (i, n, s) in [(1, \"Original\", samples_1), (2, \"Assign non-zero\", samples_2), (3, \"Drop low\", samples_3)]:\n",
    "    steering = map(lambda i: i['steering'], s)\n",
    "\n",
    "    a = np.array(steering)\n",
    "\n",
    "    print \"{}: min={}, max={}, mean={}, sigma={}\".format(n, a.min(), a.max(), a.mean(), a.std())\n",
    "\n",
    "    plt.subplot(1,3,i)\n",
    "\n",
    "    plt.text(0, 0, n, \n",
    "             fontsize=11, fontweight='bold',\n",
    "             horizontalalignment='center', \n",
    "             verticalalignment='top')\n",
    "\n",
    "    # Check training data balance.\n",
    "    plt.hist(steering, bins=51)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize 5 random signs to check the preprocessed data\n",
    "%matplotlib inline\n",
    "\n",
    "samples = samples_3\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        index = np.random.randint(0,len(samples)-1)\n",
    "\n",
    "        plt.subplot(i+1,3,j+1)\n",
    "        plt.imshow(plt.imread(samples[index]['center']))\n",
    "\n",
    "        plt.text(320/2, 0, \"{:.4f}\".format(samples[index]['steering']), \n",
    "                 fontsize=11, fontweight='bold',\n",
    "                 horizontalalignment='center', \n",
    "                 verticalalignment='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize 5 random signs to check the preprocessed data\n",
    "%matplotlib inline\n",
    "\n",
    "samples = samples_3\n",
    "\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0,len(samples)-1)\n",
    "\n",
    "    plt.figure(figsize=(12,3))\n",
    "\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.text(0.5, 0.5, \"{:.4f}\".format(samples[index]['steering']), \n",
    "             fontsize=18, fontweight='bold',\n",
    "             horizontalalignment='center', \n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(plt.imread(samples[index]['left']))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(plt.imread(samples[index]['center']))\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(plt.imread(samples[index]['right']))\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    for j in range(2):\n",
    "        for k in range(4):\n",
    "            (imgs, angs) = generator([samples[index]], batch_size=1).next()\n",
    "            plt.subplot(j+1,4,k+1)\n",
    "            plt.imshow(imgs[0])            \n",
    "            plt.text(320/2, 0, \"{:.4f}\".format(angs[0]), \n",
    "                     fontsize=11, fontweight='bold',\n",
    "                     horizontalalignment='center', \n",
    "                     verticalalignment='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Behavioral Cloning**\n",
    "\n",
    "## Behavioral Cloning Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* *Use the simulator to collect data of good driving behavior*\n",
    "  - **Actually, I was using the data provided by Udacity for the project.**\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[good_samples]: ./images/good_samples.png \"Example of Good Driving #1\"\n",
    "[good_samples2]: ./images/good_samples2.png \"Example of Good Driving #2\"\n",
    "[good_samples3]: ./images/good_samples3.png \"Example of Good Driving #3\"\n",
    "[sample_orig]: ./images/sample_orig.png \"Original sample\"\n",
    "[sample_augment]: ./images/sample_augment.png \"Augmented samples\"\n",
    "[steering_distribution]: ./images/steering_distribution.png \"Steering Distribution\"\n",
    "\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "My project includes the following files:\n",
    "* **model.py** - containing the script to create and train the model\n",
    "* **model.log** - log of model.py run.\n",
    "* **drive.py** - for driving the car in autonomous mode (has not changed)\n",
    "* **model.h5** - containing a trained convolution neural network\n",
    "* **writeup_report.md** - summarizing the results (this document)\n",
    "\n",
    "Just in case I have posted a video of how the silutator is performing using my model: https://youtu.be/D_SmuZn1td0\n",
    "\n",
    "#### 2. Submssion includes functional code\n",
    "\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing\n",
    "\n",
    "```\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "##### 2.1. Old Hardware (GTX 680)\n",
    "\n",
    "I was training the model on my workstation that is having an old (**GTX 680**) GPU that does not seem to be supported by modern TensorFlow (or at least I have not spent enough time to make it work). For this reason I am using a docker image of tensorflow that does support my GPU.\n",
    "\n",
    "```\n",
    "nvidia-docker run -d -p 8888:8888 -p 6006:6006 -p 4567:4567 -v {PWD}:/notebooks gcr.io/tensorflow/tensorflow:latest-gpu\n",
    "```\n",
    "\n",
    "This image is having **Python 2.7** so the code is adoped to work in this environment.\n",
    "\n",
    "Atop on the mentioned docker image I had had to install the following packages\n",
    "\n",
    "```\n",
    "root@284cf2c1c1c5:/notebooks# pip install eventlet\n",
    "root@284cf2c1c1c5:/notebooks# pip install Pillow\n",
    "root@284cf2c1c1c5:/notebooks# pip install flask\n",
    "root@284cf2c1c1c5:/notebooks# pip install keras\n",
    "root@284cf2c1c1c5:/notebooks# pip install h5py\n",
    "root@284cf2c1c1c5:/notebooks# pip install scikit-image\n",
    "root@284cf2c1c1c5:/notebooks# pip install pandas\n",
    "\n",
    "root@284cf2c1c1c5:/notebooks# apt-get update\n",
    "root@284cf2c1c1c5:/notebooks# apt-get install python-opencv\n",
    "root@284cf2c1c1c5:/notebooks# apt-get install python-tk\n",
    "```\n",
    "\n",
    "*To prove that it actually work I am attaching the model.log*\n",
    "\n",
    "\n",
    "#### 3. Submssion code is usable and readable\n",
    "\n",
    "The model.py file contains the code for training and saving the neural network. The file shows the pipeline I used for training and validating the model and comments to explain how the code works.\n",
    "\n",
    "##### 3.1. Model Architecture and Training Strategy\n",
    "\n",
    "###### 3.1.1. Adjusted Nvidia Model\n",
    "\n",
    "The model is mostly based on the famous Nvidia paper (https://arxiv.org/abs/1604.07316). The number of layers, number of features on each layer and number of neurons in the dense part of the network is taken from that paper. \n",
    "\n",
    "The difference is mostly in the first convolution layer that is doing 7x7 convolution and 3x3 maxpooling and the last convolution layer that is doing 3x2 convolution.\n",
    "\n",
    "Such approach was used to avoid necessity for scailing. And to make at least some difference from the Nvidia's model...\n",
    "\n",
    "###### 3.1.2. Model Construction\n",
    "\n",
    "The model construction is done by **build_model()** function (model.py:154). It is using Keras framework that has a very expressive, powerful and easy to use means. So, the architecture of the model below maps one-to-one to the code. Still the comments are provided just in case.\n",
    "\n",
    "| Layer (type)                    | Output Shape      | Param #     |\n",
    "|---------------------------------|-------------------|-------------|\n",
    "| Input                           |  3 @ 320 x 160    |             |\n",
    "|                                 |                   |             |\n",
    "| Cropping2D (t=50,b=20,l=1,r=1)  |  3 @ 318 x 90     |  0          |\n",
    "| Lambda ((rgb / 255.) - 0.5)     |  3 @ 318 x 90     |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (7x7)             |  24 @ 312 x 84    |  3552       |\n",
    "| MaxPooling2D (3x3)              |  24 @ 104 x 28    |  0          |\n",
    "| Dropout (0.5)                   |  24 @ 104 x 28    |  0          |\n",
    "| SpatialDropout2D (0.2)          |  24 @ 104 x 28    |  0          |\n",
    "| Activation (ELU)                |  24 @ 104 x 28    |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (5x5)             |  36 @ 100 x 24    |  21636      |\n",
    "| MaxPooling2D (2x2)              |  36 @ 50 x 12     |  0          |\n",
    "| Dropout (0.5)                   |  36 @ 50 x 12     |  0          |\n",
    "| SpatialDropout2D (0.2)          |  36 @ 50 x 12     |  0          |\n",
    "| Activation (ELU)                |  36 @ 50 x 12     |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (5x5)             |  48 @ 46 x 8      |  43248      |\n",
    "| MaxPooling2D (2x2)              |  48 @ 23 x 4      |  0          |\n",
    "| Dropout (0.5)                   |  48 @ 23 x 4      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  48 @ 23 x 4      |  0          |\n",
    "| Activation (ELU)                |  48 @ 23 x 4      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (3x3)             |  64 @ 21 x 2      |  27712      |\n",
    "| Dropout (0.5)                   |  64 @ 21 x 2      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  64 @ 21 x 2      |  0          |\n",
    "| Activation (ELU)                |  64 @ 21 x 2      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (3x2)             |  64 @ 19 x 1      |  24640      |\n",
    "| Dropout (0.5)                   |  64 @ 19 x 1      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  64 @ 19 x 1      |  0          |\n",
    "| Activation (ELU)                |  64 @ 19 x 1      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Flatten                         |  1216             |  0          |\n",
    "| Dropout (0.5)                   |  1216             |  0          |\n",
    "| Activation (ELU)                |  1216             |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (100)                     |  100              |  121700     |\n",
    "| Activation (ELU)                |  100              |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (50)                      |  50               |  5050       |\n",
    "| Activation (ELU)                |  50               |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (10)                      |  10               |  510        |\n",
    "| Dropout (0.5)                   |  10               |  0          |\n",
    "| Activation (ELU)                |  10               |  0          |\n",
    "|                                 |                   |             |\n",
    "| Output                          |  1                |  11         |\n",
    "\n",
    "###### 3.1.3. Preprocessing Layers\n",
    "\n",
    "Model receives the image from the simulator as-is, so whole the required preprocessing is happening in the first layers on the network (before the convolution layers).\n",
    "\n",
    "Fristly, the image is cropped to 318x90 (1px from each side, 50px from the top, and 20px from the buttom). This is done in order to:\n",
    "* Remove notinformative parts of the image (top and bottom)\n",
    "* Adjust the size of the image to suite well for the convolution / maxpooling operations.\n",
    "\n",
    "Secondly, the color values are adjusted to fit into [-1,1] range by a Lambda layer.\n",
    "\n",
    "###### 3.1.4. Reduce Overfitting\n",
    "\n",
    "To reduce overfitting and generalize better the following is done:\n",
    "* **50% dropout** are done on every convolution layer and on the first and last dense layers.\n",
    "* **20% spatial dropouts** are done on every convolution layer.\n",
    "\n",
    "The underrepresented data (samples with non-zero steering) are triplicated that provides a better mix for data for training. Note that augmenation / randomisation of the data help not to have exactly the same multiplied sample.\n",
    "\n",
    "Obviously, the model was trained and validated on different data sets to ensure that the model was not overfitting. \n",
    "\n",
    "###### 3.1.5. Appropriate Training Data\n",
    "\n",
    "The trick used by the Nvidia paper is used here as well. Namely, left and right looking camera images are used to generate the \"recovery samples\" Namely, if the image from the left camera sample is actually seen on the center camera, it means that the steering should be adjusted to the right; and if the image from the right camera is seen on the center camera, the steering should be adjusted to the left.\n",
    "\n",
    "The amount of the adjustment was a matter of experiments, but eventually I come up with **+/-0.20**.\n",
    "\n",
    "This code is implemented by **generator()** function (model.py:67)\n",
    "\n",
    "Also, there are too many zero steering samples in the training set, so assigning a reasonable non-zero steering is a good idea (if possible). The assumption here is that a non-zero steering command should mostly be applicable to the previous sample (with zero steering) as well. So, for the zero steering sample that is right before a non-zero steering will be assigned 80% of the non-zero steering.\n",
    "\n",
    "The code is implemented by **assign_non_zero()** function (model.py:34)\n",
    "\n",
    "After that the non-zero samples from the training data are triplicated (added to times to the training set). That makes zero steering samples 3 times less represented keeping its diversity. As for triplicated samples, Data augmentation implemented in **generator()** will randomize it as needed.\n",
    "\n",
    "The triplication logic is implemented by **triple_non_zero()** frunction (model.py:56)\n",
    "\n",
    "The plots below show the breakdown of the number of samples by steering original, after assigning non-zero an triplicating non zero.\n",
    "\n",
    "![alt text][steering_distribution]\n",
    "\n",
    "Last thing to mentioned is that I have decided **not to use additional training data**, so only the training data provided by Udacity for the project was used. Having less training data makes problem harder; Besides, this was it is easier to compare the result with others.\n",
    "\n",
    "##### Training Strategy\n",
    "\n",
    "For safer training ELU activation function is used (instead of ReLU) as it is able to learn even for negative weighted sum.\n",
    "\n",
    "The model used an 'adam' optimizer, so the learning rate was not tuned manually. See **train()** function (model.py:265).\n",
    "\n",
    "###### Solution Design Approach and Trial and Errors\n",
    "\n",
    "I strongly believe that the Nvidia's approach is very relevant for this case, so there were no many attempts, trial and errors considering the basic model. The only actual problem I was solving was \"to scale or not to scale\". Eventually I have decided not to scale and changed the first convolutional layer of the network.\n",
    "\n",
    "The more complex part was how to use the training data. I have explained above the operations I have eventually implemented. My experiments shows that these are enough for the task.\n",
    "\n",
    "During the process I have had few problems that are worth noting:\n",
    "* Initially I was reading pictures using **cv2.imread** that seems to encode colors differently then used in drive.py. Replacing it with **plt.imread** fixed the problem.\n",
    "* I am new to **keras** and I have not spotted that Keras' Dropout layer and TensorFlow dropout has different meaning for the argument. So I was training to train the network having 0.95 dropping probability.\n",
    "  - the most surprising thing was that it could actually learn something...\n",
    "* I spent some time choosing proper steering adjust values for the left and right cameras. Eventually I come up with +/-0.20.\n",
    "  - I tried 0.25, but the car started making too much of zigzags on the road.\n",
    "* I played with proportion of the central camera examples and side camera with adjustment. \n",
    "  - when I was doing too much of straight examples, it missed the turn below.\n",
    "  - when I was doing too few of the straight examples, again, too many zigzags.\n",
    "\n",
    "\n",
    "Also I faced a very unpleasent problem with the simulator. It looks like it is using local to generate the output and to parse the command from driver.py. In my native local (Russian) we are using comma(,) (vs period(.)) to separate the fractions in numbers. As the output format for simulator is CSV, the result is unusable for training, so I waisted about an hour of recording.\n",
    "\n",
    "It also affects parsing the commands, so the Autonomous Mode in the Simulator did not work for me from the beginnig.\n",
    "\n",
    "For the record, I have used it by to fix the problem\n",
    "```\n",
    "unset LANG\n",
    "open ./beta_simulator_mac.app\n",
    "```\n",
    "\n",
    "###### Creation of the Training Set & Training Process\n",
    "\n",
    "As it was mentioned above, I was using the data set provided by Udacity along with the projects. It contains 8036 samples that is enough for the training. \n",
    "\n",
    "To capture good driving behavior, I was using the data set. Here is an example image of center lane driving. The images are taken from the the central camera. The number on a picture is the steering angle on the sample. Some samples required some turning the steering wheel, but still it is a normal center lane driving.\n",
    "\n",
    "![alt text][good_samples]\n",
    "![alt text][good_samples2]\n",
    "![alt text][good_samples3]\n",
    "\n",
    "In order to train the model not only on good cases (when no recovery action is usually needed), but on the situations requiring recovery actions, I was using left and right cameras. This is the Nvidia trick that was mentioned above and that allows to treat side cameras as central while (after adjusting the steering) perform the recovery actions.\n",
    "\n",
    "There is the images of all 3 cameras for a sample. It is clear that the left camera does require some actions. \n",
    "\n",
    "![alt text][sample_orig]\n",
    "\n",
    "To make things clear, recovery actions were not added to the training set using a special procedure. Instead they was added to the training set a result of randomisation / augmentation of the training set.\n",
    "\n",
    "To augment the data set, I flipped images vertically and negated angles thinking that this would get valid new samples. For example, here is image after augmentation of the above original sample.\n",
    "\n",
    "![alt text][sample_augment]\n",
    "\n",
    "Also I moved the image +/-1 px by both axis. It is hard to see the difference on the picture, but it does result in slightly different pictures. \n",
    "\n",
    "After the augmentation process, I had **15926** data points.\n",
    "```\n",
    "Original:       \t total: 8036  (non-zero: 3675)\n",
    "Assign non-zero:\t total: 8036  (non-zero: 3945)\n",
    "Drop low:       \t total: 15926 (non-zero: 11835)\n",
    "```\n",
    "\n",
    "I finally randomly shuffled the data set and put **20%** of the data into a validation set.\n",
    "```\n",
    "Training: 12740, Validation: 3186\n",
    "```\n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was **35** as evidenced by the fact that starting this moment the loss of the validation set stopped improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Behavioral Cloning**\n",
    "\n",
    "## Behavioral Cloning Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* *Use the simulator to collect data of good driving behavior*\n",
    "  - **Actually, I was using the data provided by Udacity for the project.**\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[good_samples]: ./images/good_samples.png \"Example of Good Driving #1\"\n",
    "[good_samples2]: ./images/good_samples2.png \"Example of Good Driving #2\"\n",
    "[good_samples3]: ./images/good_samples3.png \"Example of Good Driving #3\"\n",
    "[sample_orig]: ./images/sample_orig.png \"Original sample\"\n",
    "[sample_augment]: ./images/sample_augment.png \"Augmented samples\"\n",
    "[steering_distribution]: ./images/steering_distribution.png \"Steering Distribution\"\n",
    "\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "My project includes the following files:\n",
    "* **model.py** - containing the script to create and train the model\n",
    "* **model.log** - log of model.py run.\n",
    "* **drive.py** - for driving the car in autonomous mode (has not changed)\n",
    "* **model.h5** - containing a trained convolutional neural network\n",
    "* **writeup_report.md** - summarizing the results (this document)\n",
    "\n",
    "Just in case I have posted a video of how the silutator is performing using my model: https://youtu.be/D_SmuZn1td0\n",
    "\n",
    "#### Submission includes functional code\n",
    "\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing\n",
    "\n",
    "```\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "##### Old Hardware (GTX 680)\n",
    "\n",
    "I was training the model on my workstation that is having an old (**GTX 680**) GPU that does not seem to be supported by modern TensorFlow (or at least I have not spent enough time to make it work). For this reason I am using a docker image of Tensorflow that does support my GPU.\n",
    "\n",
    "```\n",
    "nvidia-docker run -d -p 8888:8888 -p 6006:6006 -p 4567:4567 -v {PWD}:/notebooks gcr.io/tensorflow/tensorflow:latest-gpu\n",
    "```\n",
    "\n",
    "This image is having **Python 2.7** so the code is adopted to work in this environment.\n",
    "\n",
    "Atop on the mentioned docker image I had had to install the following packages\n",
    "\n",
    "```\n",
    "root@284cf2c1c1c5:/notebooks# pip install eventlet\n",
    "root@284cf2c1c1c5:/notebooks# pip install Pillow\n",
    "root@284cf2c1c1c5:/notebooks# pip install flask\n",
    "root@284cf2c1c1c5:/notebooks# pip install keras\n",
    "root@284cf2c1c1c5:/notebooks# pip install h5py\n",
    "root@284cf2c1c1c5:/notebooks# pip install scikit-image\n",
    "root@284cf2c1c1c5:/notebooks# pip install pandas\n",
    "\n",
    "root@284cf2c1c1c5:/notebooks# apt-get update\n",
    "root@284cf2c1c1c5:/notebooks# apt-get install python-opencv\n",
    "root@284cf2c1c1c5:/notebooks# apt-get install python-tk\n",
    "```\n",
    "\n",
    "*To prove that it actually work I am attaching the model.log*\n",
    "\n",
    "\n",
    "#### Submission code is usable and readable\n",
    "\n",
    "The model.py file contains the code for training and saving the neural network. The file shows the pipeline I used for training and validating the model and comments to explain how the code works.\n",
    "\n",
    "##### Model Architecture and Training Strategy\n",
    "\n",
    "###### Adjusted Nvidia Model\n",
    "\n",
    "The model is mostly based on the famous Nvidia paper (https://arxiv.org/abs/1604.07316). The number of layers, number of features on each layer and number of neurons in the dense part of the network is taken from that paper.\n",
    "\n",
    "The difference is mostly in the first convolution layer that is doing 7x7 convolution and 3x3 maxpooling and the last convolution layer that is doing 3x2 convolution.\n",
    "\n",
    "Such approach was used to avoid necessity for scaling. And to make at least some difference from the Nvidia's model...\n",
    "\n",
    "###### Model Construction\n",
    "\n",
    "The model construction is done by **build_model()** function (model.py:154). It is using Keras framework that has a very expressive, powerful and easy to use means. So, the architecture of the model below maps one-to-one to the code. Still the comments are provided just in case.\n",
    "\n",
    "| Layer (type)                    | Output Shape      | Param #     |\n",
    "|---------------------------------|-------------------|-------------|\n",
    "| Input                           |  3 @ 320 x 160    |             |\n",
    "|                                 |                   |             |\n",
    "| Cropping2D (t=50,b=20,l=1,r=1)  |  3 @ 318 x 90     |  0          |\n",
    "| Lambda ((rgb / 255.) - 0.5)     |  3 @ 318 x 90     |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (7x7)             |  24 @ 312 x 84    |  3552       |\n",
    "| MaxPooling2D (3x3)              |  24 @ 104 x 28    |  0          |\n",
    "| Dropout (0.5)                   |  24 @ 104 x 28    |  0          |\n",
    "| SpatialDropout2D (0.2)          |  24 @ 104 x 28    |  0          |\n",
    "| Activation (ELU)                |  24 @ 104 x 28    |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (5x5)             |  36 @ 100 x 24    |  21636      |\n",
    "| MaxPooling2D (2x2)              |  36 @ 50 x 12     |  0          |\n",
    "| Dropout (0.5)                   |  36 @ 50 x 12     |  0          |\n",
    "| SpatialDropout2D (0.2)          |  36 @ 50 x 12     |  0          |\n",
    "| Activation (ELU)                |  36 @ 50 x 12     |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (5x5)             |  48 @ 46 x 8      |  43248      |\n",
    "| MaxPooling2D (2x2)              |  48 @ 23 x 4      |  0          |\n",
    "| Dropout (0.5)                   |  48 @ 23 x 4      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  48 @ 23 x 4      |  0          |\n",
    "| Activation (ELU)                |  48 @ 23 x 4      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (3x3)             |  64 @ 21 x 2      |  27712      |\n",
    "| Dropout (0.5)                   |  64 @ 21 x 2      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  64 @ 21 x 2      |  0          |\n",
    "| Activation (ELU)                |  64 @ 21 x 2      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Convolution2D (3x2)             |  64 @ 19 x 1      |  24640      |\n",
    "| Dropout (0.5)                   |  64 @ 19 x 1      |  0          |\n",
    "| SpatialDropout2D (0.2)          |  64 @ 19 x 1      |  0          |\n",
    "| Activation (ELU)                |  64 @ 19 x 1      |  0          |\n",
    "|                                 |                   |             |\n",
    "| Flatten                         |  1216             |  0          |\n",
    "| Dropout (0.5)                   |  1216             |  0          |\n",
    "| Activation (ELU)                |  1216             |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (100)                     |  100              |  121700     |\n",
    "| Activation (ELU)                |  100              |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (50)                      |  50               |  5050       |\n",
    "| Activation (ELU)                |  50               |  0          |\n",
    "|                                 |                   |             |\n",
    "| Dense (10)                      |  10               |  510        |\n",
    "| Dropout (0.5)                   |  10               |  0          |\n",
    "| Activation (ELU)                |  10               |  0          |\n",
    "|                                 |                   |             |\n",
    "| Output                          |  1                |  11         |\n",
    "\n",
    "###### Preprocessing Layers\n",
    "\n",
    "Model receives the image from the simulator as-is, so whole the required preprocessing is happening in the first layers on the network (before the convolution layers).\n",
    "\n",
    "Firstly, the image is cropped to 318x90 (1px from each side, 50px from the top, and 20px from the bottom). This is done in order to:\n",
    "* Remove non-informative parts of the image (top and bottom)\n",
    "* Adjust the size of the image to suite well for the convolution / maxpooling operations.\n",
    "\n",
    "Secondly, the color values are adjusted to fit into [-1,1] range by a Lambda layer.\n",
    "\n",
    "###### 3.1.4. Reduce Overfitting\n",
    "\n",
    "To reduce overfitting and generalize better the following is done:\n",
    "* **50% dropout** are done on every convolution layer and on the first and last dense layers.\n",
    "* **20% spatial dropouts** are done on every convolution layer.\n",
    "\n",
    "The underrepresented data (samples with non-zero steering) are triplicated that provides a better mix for data for training. Note that augmenation / randomisation of the data help not to have exactly the same multiplied sample.\n",
    "\n",
    "Obviously, the model was trained and validated on different data sets to ensure that the model was not overfitting.\n",
    "\n",
    "###### Appropriate Training Data\n",
    "\n",
    "The trick used by the Nvidia paper is used here as well. Namely, left and right looking camera images are used to generate the \"recovery samples\" Namely, if the image from the left camera sample is actually seen on the center camera, it means that the steering should be adjusted to the right; and if the image from the right camera is seen on the center camera, the steering should be adjusted to the left.\n",
    "\n",
    "The amount of the adjustment was a matter of experiments, but eventually I come up with **+/-0.20**.\n",
    "\n",
    "This code is implemented by **generator()** function (model.py:67)\n",
    "\n",
    "Also, there are too many zero steering samples in the training set, so assigning a reasonable non-zero steering is a good idea (if possible). The assumption here is that a non-zero steering command should mostly be applicable to the previous sample (with zero steering) as well. So, for the zero steering sample that is right before a non-zero steering will be assigned 80% of the non-zero steering.\n",
    "\n",
    "The code is implemented by **assign_non_zero()** function (model.py:34)\n",
    "\n",
    "After that the non-zero samples from the training data are triplicated (added to times to the training set). That makes zero steering samples 3 times less represented keeping its diversity. As for triplicated samples, Data augmentation implemented in **generator()** will randomize it as needed.\n",
    "\n",
    "The triplication logic is implemented by **triple_non_zero()** frunction (model.py:56)\n",
    "\n",
    "The plots below show the breakdown of the number of samples by steering original, after assigning non-zero an triplicating non zero.\n",
    "\n",
    "![alt text][steering_distribution]\n",
    "\n",
    "Last thing to mention is that I have decided **not to use additional training data**, so only the training data provided by Udacity for the project was used. Having less training data makes problem harder; Besides, this was it is easier to compare the result with others.\n",
    "\n",
    "##### Training Strategy\n",
    "\n",
    "For safer training ELU activation function is used (instead of ReLU) as it is able to learn even for negative weighted sum.\n",
    "\n",
    "The model used an 'adam' optimizer, so the learning rate was not tuned manually. See **train()** function (model.py:265).\n",
    "\n",
    "###### Solution Design Approach and Trial and Errors\n",
    "\n",
    "I strongly believe that the Nvidia's approach is very relevant for this case, so there were no many attempts, trial and errors considering the basic model. The only actual problem I was solving was \"to scale or not to scale\". Eventually I have decided not to scale and changed the first convolutional layer of the network.\n",
    "\n",
    "The more complex part was how to use the training data. I have explained above the operations I have eventually implemented. My experiments shows that these are enough for the task.\n",
    "\n",
    "During the process I have had few problems that are worth noting:\n",
    "* Initially I was reading pictures using **cv2.imread** that seems to encode colors differently then used in drive.py. Replacing it with **plt.imread** fixed the problem.\n",
    "* I am new to **keras** and I have not spotted that Keras' Dropout layer and TensorFlow dropout has different meaning for the argument. So I was training to train the network having 0.95 dropping probability.\n",
    "  - the most surprising thing was that it could actually learn something...\n",
    "* I spent some time choosing proper steering adjust values for the left and right cameras. Eventually I come up with +/-0.20.\n",
    "  - I tried 0.25, but the car started making too much of zigzags on the road.\n",
    "* I played with proportion of the central camera examples and side camera with adjustment.\n",
    "  - when I was doing too much of straight examples, it missed the turn below.\n",
    "  - when I was doing too few of the straight examples, again, too many zigzags.\n",
    "\n",
    "\n",
    "Also I faced a very unpleasant problem with the simulator. It looks like it is using local to generate the output and to parse the command from driver.py. In my native local (Russian) we are using comma(,) (vs period(.)) to separate the fractions in numbers. As the output format for simulator is CSV, the result is unusable for training, so I wasted about an hour of recording.\n",
    "\n",
    "It also affects parsing the commands, so the Autonomous Mode in the Simulator did not work for me from the beginning.\n",
    "\n",
    "For the record, I have used it by to fix the problem\n",
    "```\n",
    "unset LANG\n",
    "open ./beta_simulator_mac.app\n",
    "```\n",
    "\n",
    "###### Creation of the Training Set & Training Process\n",
    "\n",
    "As it was mentioned above, I was using the data set provided by Udacity along with the projects. It contains 8036 samples that is enough for the training.\n",
    "\n",
    "To capture good driving behavior, I was using the data set. Here is an example image of center lane driving. The images are taken from the the central camera. The number on a picture is the steering angle on the sample. Some samples required some turning the steering wheel, but still it is a normal center lane driving.\n",
    "\n",
    "![alt text][good_samples]\n",
    "![alt text][good_samples2]\n",
    "![alt text][good_samples3]\n",
    "\n",
    "In order to train the model not only on good cases (when no recovery action is usually needed), but on the situations requiring recovery actions, I was using left and right cameras. This is the Nvidia trick that was mentioned above and that allows to treat side cameras as central while (after adjusting the steering) perform the recovery actions.\n",
    "\n",
    "There is the images of all 3 cameras for a sample. It is clear that the left camera does require some actions.\n",
    "\n",
    "![alt text][sample_orig]\n",
    "\n",
    "To make things clear, recovery actions were not added to the training set using a special procedure. Instead they was added to the training set a result of randomisation / augmentation of the training set.\n",
    "\n",
    "To augment the data set, I flipped images vertically and negated angles thinking that this would get valid new samples. For example, here is image after augmentation of the above original sample.\n",
    "\n",
    "![alt text][sample_augment]\n",
    "\n",
    "Also I moved the image +/-1 px by both axis. It is hard to see the difference on the picture, but it does result in slightly different pictures.\n",
    "\n",
    "After the augmentation process, I had **15926** data points.\n",
    "```\n",
    "Original:\t\t  total: 8036  (non-zero: 3675)\n",
    "Assign non-zero:  total: 8036  (non-zero: 3945)\n",
    "Drop low:         total: 15926 (non-zero: 11835)\n",
    "```\n",
    "\n",
    "I finally randomly shuffled the data set and put **20%** of the data into a validation set.\n",
    "```\n",
    "Training: 12740, Validation: 3186\n",
    "```\n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was **35** as evidenced by the fact that starting this moment the loss of the validation set stopped improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
